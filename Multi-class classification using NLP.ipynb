{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras import models,layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentiment:             # classes for getting sentiments and categories\n",
    "    positive = 'positive'\n",
    "    negative = 'negative'           \n",
    "    neutral = 'neutral'\n",
    "class Review:\n",
    "    def __init__(self,text,rating,category):\n",
    "        self.text = text\n",
    "        self.rating = rating\n",
    "        self.category = category\n",
    "        self.sentiment = self.get_sentiment()\n",
    "    def get_sentiment(self):\n",
    "        if self.rating<=2:\n",
    "            return sentiment.negative\n",
    "        elif self.rating==3:\n",
    "            return sentiment.neutral\n",
    "        else:\n",
    "            return sentiment.positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/category' # file-path\n",
    "categories = []\n",
    "for f in os.listdir(file_path):\n",
    "    categories.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading data in reviews(List) through Review class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "for file in os.listdir(file_path):\n",
    "    path = os.path.join(file_path,file)\n",
    "    with open(path,'r') as f:\n",
    "        for line in f:\n",
    "            review = json.loads(line)\n",
    "            reviews.append(Review(review['reviewText'],review['overall'],\n",
    "                                  categories[categories.index(file)].split('_')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # for splitting train and test data\n",
    "train_data,test_data = train_test_split(reviews,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [i.text for i in train_data] # get sentences/reviews\n",
    "test_texts = [i.text for i in test_data]\n",
    "train_labels = [i.category for i in train_data] # get categories\n",
    "test_labels = [i.category for i in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=20000\n",
    "oov_token='##'\n",
    "max_length=140\n",
    "embed_dim=32\n",
    "\n",
    "# Tokenizer class for tokenizing text reviews\n",
    "tokenizer = Tokenizer(num_words=vocab_size,oov_token=oov_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I definitely recommend it to anyone who like to watch videos on the big screen like July Plus, Netflix, YouTube. Is easy to set up and fast the installing.\n",
      "[3, 238, 129, 7, 5, 386, 117, 27, 5, 371, 1684, 19, 2, 194, 411, 27, 4124, 492, 2439, 2138, 9, 84, 5, 214, 45, 4, 395, 2, 2270]\n",
      "[   3  238  129    7    5  386  117   27    5  371 1684   19    2  194\n",
      "  411   27 4124  492 2439 2138    9   84    5  214   45    4  395    2\n",
      " 2270    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(train_texts)\n",
    "indexes = tokenizer.word_index\n",
    "seq = tokenizer.texts_to_sequences(train_texts)\n",
    "pad = pad_sequences(seq,maxlen=max_length,padding='post',truncating='post')\n",
    "print(train_texts[0])\n",
    "print(seq[0])\n",
    "print(pad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder # label encoder which transforms output text labels into integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_labels)\n",
    "encoded_Y = encoder.transform(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = encoder.fit_transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_texts)\n",
    "test_pad = pad_sequences(test_seq,maxlen=max_length,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical # one hot encoding\n",
    "tr_labels = to_categorical(encoded_Y, 5)\n",
    "te_labels = to_categorical(Y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential() # create model\n",
    "model.add(layers.Embedding(input_dim=vocab_size,output_dim=embed_dim,input_length=max_length))\n",
    "model.add(layers.GlobalAveragePooling1D())\n",
    "model.add(layers.Dense(32,activation='relu'))\n",
    "model.add(layers.Dense(5,activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.2575 - acc: 0.9631 - val_loss: 0.4066 - val_acc: 0.8820\n",
      "Epoch 2/15\n",
      "110/110 [==============================] - 1s 14ms/step - loss: 0.2100 - acc: 0.9666 - val_loss: 0.3696 - val_acc: 0.9007\n",
      "Epoch 3/15\n",
      "110/110 [==============================] - 1s 14ms/step - loss: 0.1735 - acc: 0.9749 - val_loss: 0.3476 - val_acc: 0.9000\n",
      "Epoch 4/15\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.1441 - acc: 0.9789 - val_loss: 0.3280 - val_acc: 0.9087\n",
      "Epoch 5/15\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.1220 - acc: 0.9826 - val_loss: 0.3165 - val_acc: 0.9067\n",
      "Epoch 6/15\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.1045 - acc: 0.9837 - val_loss: 0.3023 - val_acc: 0.9033\n",
      "Epoch 7/15\n",
      "110/110 [==============================] - 1s 14ms/step - loss: 0.0899 - acc: 0.9854 - val_loss: 0.2955 - val_acc: 0.9060\n",
      "Epoch 8/15\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.0786 - acc: 0.9897 - val_loss: 0.2885 - val_acc: 0.9093\n",
      "Epoch 9/15\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.0680 - acc: 0.9906 - val_loss: 0.2890 - val_acc: 0.9053\n",
      "Epoch 10/15\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.0600 - acc: 0.9911 - val_loss: 0.2850 - val_acc: 0.9067\n",
      "Epoch 11/15\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.0537 - acc: 0.9920 - val_loss: 0.2934 - val_acc: 0.9040\n",
      "Epoch 12/15\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.0473 - acc: 0.9949 - val_loss: 0.2766 - val_acc: 0.9067\n",
      "Epoch 13/15\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.0427 - acc: 0.9949 - val_loss: 0.2767 - val_acc: 0.9073\n",
      "Epoch 14/15\n",
      "110/110 [==============================] - 1s 14ms/step - loss: 0.0372 - acc: 0.9957 - val_loss: 0.2792 - val_acc: 0.9073\n",
      "Epoch 15/15\n",
      "110/110 [==============================] - 1s 14ms/step - loss: 0.0343 - acc: 0.9946 - val_loss: 0.2783 - val_acc: 0.9067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x216dbbb7070>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(pad,tr_labels,epochs=15,validation_data=(test_pad,te_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict([test_pad]) # predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(predictions[0]))\n",
    "print(np.argmax(predictions[1]))\n",
    "print(np.argmax(predictions[2]))\n",
    "print(np.argmax(predictions[3]))\n",
    "print(np.argmax(predictions[4]))\n",
    "print(np.argmax(predictions[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electronics\n",
      "Books\n",
      "Grocery\n",
      "Electronics\n",
      "Electronics\n",
      "Electronics\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0].category)\n",
    "print(test_data[1].category)\n",
    "print(test_data[2].category)\n",
    "print(test_data[3].category)\n",
    "print(test_data[4].category)\n",
    "print(test_data[7].category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
